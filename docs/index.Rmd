---
title: "Toronto AirBNB Analysis"
output:
  html_document:
    df_print: paged
---

```{r include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

```

```{r}
# Load packages
library(here)
library(tidyverse)
library(gt)
library(e1071)
library(scales)
library(corrplot)
library(caret)
library(randomForest)
library(glmnet)
library(gbm)
```


```{r}
# Read in data
data  <- read_csv(here("inputs//data_prep.csv"))
data <- data %>%
  mutate(room_type = as.factor(room_type), license = as.factor(license))
```
# Introduction

For this analysis we're explore the listings data of Airbnb rentals in Toronto. The data can be found at [link](http://insideairbnb.com/get-the-data/)

We're primarily interested in the rental price however we'll explore the entire dataset for anything interesting and visualize the results. The analysis will conclude with a model for predicting rental prices.

We've previously removed all listings with no reviews, we also remove all listings with no availability within the next year as these are likely no longer actively being rented and removed NA columns from the data.

# Data Exploration

We'll generate some initial summary statistics of the various predictors
to get started:


```{r}
# Check Summary Statistics
summary(data)
```
Some initial things to note from the summary statistics is that the vast majority of listings(67.7%) are entire homes or apartments as opposed to shared living spaces. Private rooms make up the bulk of the remainder at 31.5%.

```{r, fig.width = 5, fig.height = 4}

data %>%
  select(room_type) %>%
  group_by(room_type) %>%
  summarize(listings = n()) %>%
  ungroup() %>%
  mutate(room_type = fct_reorder(room_type, listings)) %>%
  ggplot(aes(x = room_type, y = listings, fill = room_type)) +
  geom_bar(stat = "identity") + 
  theme(legend.position = "none") +
  xlab("Room Type") +
  ylab("Listings")
```

Like any other residential property the neighbourhood is a likely predictor of the price of the property being rented so next be look at the distribution of 

Lets now look at the distribution of rental prices:

```{r, fig.width = 5, fig.height = 4}
ggplot(data, aes(x = price)) +
  geom_histogram(binwidth = 10, fill = "blue", color = "black") +
  labs(title = "Distribution of Airbnb Rental Prices",
       x = "Price",
       y = "Count")
```

I've identified the major outlier in this case to be [this](https://www.airbnb.ca/rooms/43356632?adults=1&enable_m3_private_room=true&check_in=2023-05-10&check_out=2023-05-12&federated_search_id=1e55edf8-ab38-416f-a94b-39b161fc71c1&source_impression_id=p3_1683402984_4BxxLqBL6P8g6lGs&guests=1) listing. To get a better idea we can check for example how many listings are there above a price of $5000?

```{r}
dim(data %>%
  filter(price > 5000))[1]
```

We find there are only 13 listings. We can then look at a boxplot of the remaining listings after removing the ones above 5000.

```{r, fig.width = 5, fig.height = 4}
data %>%
  filter(price < 5000) %>%
  ggplot(aes(x = price)) +
  geom_boxplot(color = "blue", outlier.color = "red", outlier.size = 2) +
  scale_x_continuous(breaks = c(0,1000,2000, 3000, 4000, 5000)) +
  ylim(-4,4) + 
  stat_boxplot(geom ='errorbar') +
  theme(axis.title.y = element_blank(),
              axis.text.y = element_blank(),
              axis.ticks.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_blank())
```

The data seems to get much more spread out above a price of $1500, so we'll focus in on those data points:

```{r, fig.width = 5, fig.height = 4}
data %>%
  filter(price < 1500) %>%
  ggplot(aes(x = price)) +
  geom_boxplot(color = "blue", outlier.color = "red", outlier.size = 2) +
  scale_x_continuous(breaks = seq(from = 0, to = 1500, by = 250)) +
  ylim(-4,4) + 
  stat_boxplot(geom ='errorbar') +
  theme(axis.title.y = element_blank(),
              axis.text.y = element_blank(),
              axis.ticks.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_blank())
```

This seems like a better representation of most of the price data for the listings.

As with any property value we may expect things like the number of bedrooms in the listing to be a predictor of price. Larger properties should have higher bedroom counts and thus be more expensive to rent:

```{r, fig.width = 5, fig.height = 4}
data %>%
  filter(price < 1500) %>%
  mutate(bedrooms = as.factor(bedrooms)) %>%
  select(price, bedrooms) %>%
  group_by(bedrooms) %>%
  summarize(mean_price = mean(price)) %>%
  ungroup() %>%
  ggplot(aes(x = bedrooms, y = mean_price, fill = bedrooms)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 250)) +
  theme(legend.position = "none") +
  xlab("# of Bedrooms") +
  ylab("Mean Price")
```

There appears to be only 1 listing with 9 bedrooms.
```{r}
data %>%
  filter(price < 1500 & bedrooms == 9)
```

Because the price is so wildly off from the overall trend I think it's best to remove this point in the model.

Next it seems likely review scores should be a strong predictor of prices:

```{r, fig.width = 5, fig.height = 4}
data %>%
  filter(price < 1500 & bedrooms < 9 & number_of_reviews > 5) %>%
  ggplot(aes(x = review_scores_rating, y = price)) +
    geom_point(alpha = 0.5, color = "#43a2ca") +
    geom_smooth(method = "loess", se = FALSE, color = "red") +
  labs(title = "Price vs. Review Scores Rating",
       x = "Review Scores",
       y = "Price")

```

We should also check for linear correlations between predictors, we can visualize
this via a correlation matrix

```{r, fig.width = 5, fig.height = 4}
df <- data %>%
  filter(price < 1500 & bedrooms < 9) %>%
  select(price, minimum_nights, number_of_reviews, review_scores_rating, calculated_host_listings_count)

corrplot(cor(as.matrix(df),method = "spearman"),
                 method = "color",
                  tl.cex = 0.9,
                 number.cex = 0.95,
                 addCoef.col = "black")
```

# Feature Selection




```{r}
df <- data %>%
  filter(price < 1500 & bedrooms < 9) %>%
  select(id, longitude, latitude, price, room_type, minimum_nights, number_of_reviews, last_review, reviews_per_month, availability_365, bedrooms, calculated_host_listings_count,
         number_of_reviews_ltm) %>% drop_na()
```

We would like to find a model that outperforms linear regression
performed using all predictors. This is the simplest model we could employ,
though we found no evidence that the predictors have linear relationships
with price it will serve as a good baseline to assess the performance of other
models which are more computationally expensive to train.

```{r}
set.seed(1)

train_data <- df %>%
  sample_frac(0.70)

test_data <- anti_join(df, train_data, by = 'id')

# Train model
mod.lm <- lm(price ~ .-id, data = train_data)

# Test model on test set
predictions <- predict(mod.lm, test_data, type = "response")

# Calculating RMSE and MAE
mae <- mean(abs(test_data$price - predictions))
cat("Mean Absolute Error (MAE):", mae, "\n")

rmse <- sqrt(mean((test_data$price - predictions)^2))
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
```

Now we may want to test some other model techniques namely ones that contain some sort of feature selection. To do that we will use random forest and lasso regression. Ideally these models will produce lower error rates then our standard
linear model as they should capture some of the non linear relationships we observed in the data exploration phase.

```{r}
set.seed(1)

# Split data train/test

train_data <- df %>%
  sample_frac(0.70)

test_data <- anti_join(df, train_data, by = 'id')

# Defining Control Parameters
control_rfe <- rfeControl(functions = rfFuncs,
                          method = "cv",
                          number = 5,
                          verbose = FALSE)

# Defining Predictors
predictors <- names(train_data[, !(names(train_data) %in% c("price"))])

# Run RFE
rfe_result <- rfe(train_data[, predictors],
                  train_data$price,
                  sizes = c(1:10),
                  rfeControl = control_rfe)
print(rfe_result)
```

The top 5 of 10 predictors are bedrooms, minimum_nights, room_type, latitude
and longitude

```{r}
set.seed(1)

best_features <- predictors[rfe_result$optVariables]

# Train the final model with the selected features
final_model <- randomForest(price ~ ., data = train_data[, c("price", rfe_result$optVariables)])

# Evaluate the model performance on the test dataset
predictions <- predict(final_model, newdata = test_data[, rfe_result$optVariables])
performance <- postResample(predictions, test_data$price)
print(performance)

```

As expected random forest performs better then the standard linear regression.
Now we build a lasso regression model.

```{r}
set.seed(1)

# Creating model matrix of predictors for Lasso fucntion
X <- model.matrix(price ~ . -1, data = train_data)
X.test <- model.matrix(price ~ . -1, data = test_data)
Y <- train_data$price
Y.test <- test_data$price

# Fit LASSO model
lasso.cv <- cv.glmnet(X, Y, alpha = 1, nfolds = 5)
# Finding optimal lambda
optimal <- lasso.cv$lambda.min
# Fit the optimal lambda model
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = optimal)

# Make predictions on new test data
lasso.pred <- predict(lasso.mod, optimal, newx = X.test)

# Compute error
mae <- mean(abs(lasso.pred-Y.test))
cat("Mean Absolute Error (MAE):", mae, "\n")

rmse <- sqrt(mean((lasso.pred-Y.test)^2))
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
```

In  this case we find the lasso not to be a significant improvement over the 
linear regression model. This is most likely caused by lasso not capturing non 
linear relationships which are captured in random forest.

The next method we will assess is gradient boosting machines

```{r}
set.seed(1)

gbm_model <- gbm(price ~ .-last_review,data = train_data,  distribution = "gaussian",
                 n.trees = 1000,
                 interaction.depth = 3,
                 shrinkage = 0.01,
                 n.minobsinnode = 10,
                 cv.folds = 5,
                 verbose = FALSE)

# Determine the optimal tree number
optimal_trees <- gbm.perf(gbm_model, method = "cv")

# Compute error rates
predictions <- predict(gbm_model, newdata = test_data, n.trees = optimal_trees)
performance <- postResample(predictions, test_data$price)
print(performance)
```

Gradient boosting produces slightly better error rates then the random forest
method. Next we will build a support vector machine model, to do this we will
normalize the features so they are all using the same scale.

```{r}
set.seed(1)

# Normalizing train and test split
preprocess_params <- preProcess(train_data[, -which(names(train_data) == "price")], method = c("center", "scale"))

train_data_scaled <- predict(preprocess_params, train_data)
test_data_scaled <- predict(preprocess_params, test_data)
test_data_scaled$price <- test_data$price

# Training model with svm()
svm_model <- svm(price ~ .,
                 data = train_data_scaled,
                 kernel = "radial",
                 cost = 1,
                 gamma = 0.1,
                 epsilon = 0.1)

# Computing error rates
predictions <- predict(svm_model, newdata = test_data_scaled)
performance <- postResample(predictions, test_data_scaled$price)
print(performance)
```

The SVM performs the best of our models, with RMSE of 108 vs the next best performing model, the random forest of 115. 

```{r}
library(ggplot2)

pred_vs_true <- data.frame(Predicted = predictions, True = test_data_scaled$price)
ggplot(pred_vs_true, aes(x = True, y = Predicted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(title = "SVM Model: Predicted vs. True Prices",
       x = "True Prices",
       y = "Predicted Prices") +
  theme_minimal()


```

